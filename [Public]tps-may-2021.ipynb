{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ground up solution","metadata":{}},{"cell_type":"markdown","source":"First of all, this solution is built on the idea to practise the complete workflow of machine learning prediction. Even though auto ML and blending other people's results are frequently used and legit in Kaggle, I think building a ground up solution is beneficial for gaining really solid understanding of the ML techniques.\n\nSince my computer is not powerful enough to handle the size of data here (100K times 50) with ease, I will strive for simple and efficient way to build the classification model on Kaggle hosting server. Again, the modelling process here is for learning the ML process rather than doing the fancy stuff or building specific solutions that are not ready for generalisation to other problems.\n\nBased on the background above, you will see a solution in favour of simple ML workflow and low computation cost, ready to be deployed for different problems. The presentation may be raw, but I will keep it to show how the result is improved gradually.\n\nWorkflow:\n1. Data Exploration\n2. Data Preprocessing\n3. Feature Engineering\n4. Feature Selection\n5. Model Validation And Selection\n6. Hyperparameter Tuning\n","metadata":{}},{"cell_type":"markdown","source":"# Preparation","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:58:55.274806Z","iopub.execute_input":"2021-05-23T06:58:55.275347Z","iopub.status.idle":"2021-05-23T06:58:55.281032Z","shell.execute_reply.started":"2021-05-23T06:58:55.275243Z","shell.execute_reply":"2021-05-23T06:58:55.279899Z"}}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesClassifier, StackingClassifier\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import RidgeClassifier, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV, LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import Pool, CatBoostClassifier\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, validation_curve\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_classif\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os\nos.listdir(\"../input/\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T01:57:03.409971Z","iopub.execute_input":"2021-05-31T01:57:03.410383Z","iopub.status.idle":"2021-05-31T01:57:05.524116Z","shell.execute_reply.started":"2021-05-31T01:57:03.410284Z","shell.execute_reply":"2021-05-31T01:57:05.523225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read data","metadata":{}},{"cell_type":"code","source":"# Read in the dataset as a dataframe\ntrain = pd.read_csv(\"../input/tabular-playground-series-may-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-may-2021/test.csv\")\nsubmission = pd.read_csv(\"../input/tabular-playground-series-may-2021/sample_submission.csv\")\n\n#train.info()\n#test.info()\n#submission.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T01:57:05.525997Z","iopub.execute_input":"2021-05-31T01:57:05.526214Z","iopub.status.idle":"2021-05-31T01:57:06.28702Z","shell.execute_reply.started":"2021-05-31T01:57:05.526192Z","shell.execute_reply":"2021-05-31T01:57:06.286496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split datasets","metadata":{}},{"cell_type":"code","source":"# Split features and labels\ntrain_labels = train['target'].reset_index(drop=True)\ntrain_features = train.drop(['id','target'], axis=1)\ntest_features = test.drop(['id'], axis=1)\ntrain_labels.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:13:43.316984Z","iopub.execute_input":"2021-05-31T02:13:43.317242Z","iopub.status.idle":"2021-05-31T02:13:43.334888Z","shell.execute_reply.started":"2021-05-31T02:13:43.31722Z","shell.execute_reply":"2021-05-31T02:13:43.33396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"markdown","source":"## Target distribution\n\nAs observed, 57% of the target in the training set is of \"Class 2\", which is moderately imbalanced.","metadata":{}},{"cell_type":"code","source":"\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.histplot(train['target'].sort_values(), color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Target\")\nax.set(title=\"Target distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:13:58.355619Z","iopub.execute_input":"2021-05-31T02:13:58.355918Z","iopub.status.idle":"2021-05-31T02:13:58.635438Z","shell.execute_reply.started":"2021-05-31T02:13:58.355896Z","shell.execute_reply":"2021-05-31T02:13:58.634558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].value_counts().sort_values(ascending=False)/sum(train['target'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:14:01.28161Z","iopub.execute_input":"2021-05-31T02:14:01.281935Z","iopub.status.idle":"2021-05-31T02:14:01.3391Z","shell.execute_reply.started":"2021-05-31T02:14:01.281909Z","shell.execute_reply":"2021-05-31T02:14:01.338232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features EDA\n\nNo specific pattern is observed in this case.","metadata":{}},{"cell_type":"code","source":"\n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train_features), 1):\n    plt.subplot(len(list(train_features)), 3, i)\n    sns.boxplot(x=feature, y=train_labels, hue=train_labels, palette='Blues', data=train_features)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('Target', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:14:13.242478Z","iopub.execute_input":"2021-05-31T02:14:13.242816Z","iopub.status.idle":"2021-05-31T02:14:32.946573Z","shell.execute_reply.started":"2021-05-31T02:14:13.242782Z","shell.execute_reply":"2021-05-31T02:14:32.945848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation\n\nFilter by RF feature importance first when the number of features is too large.\n\nThe 50 features show no significant correlation with each other.","metadata":{}},{"cell_type":"code","source":"\n# Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\nrf_model = rf.fit(train_features, train_labels)\n#rf_pred = rf_model.predict_proba(test_features)\n\nforest_importances = pd.Series(rf.feature_importances_, index=train_features.columns)\ntop_feat = forest_importances.sort_values(ascending = False).head(20)\ntop_feat\n\ntrain_features[top_feat.index]\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:16:53.810193Z","iopub.execute_input":"2021-05-31T02:16:53.81049Z","iopub.status.idle":"2021-05-31T02:17:25.502656Z","shell.execute_reply.started":"2021-05-31T02:16:53.810444Z","shell.execute_reply":"2021-05-31T02:17:25.502194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#corr = train_features[top_feat.index].corr()\n#corr\ncorr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:17:25.50348Z","iopub.execute_input":"2021-05-31T02:17:25.503757Z","iopub.status.idle":"2021-05-31T02:17:28.01473Z","shell.execute_reply.started":"2021-05-31T02:17:25.503736Z","shell.execute_reply":"2021-05-31T02:17:28.014243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Further exploration for high correlation to target\n\nThe most important features by RF is feature_38, but visually its standalone correlation with the target is insignificant.","metadata":{}},{"cell_type":"code","source":"\ndata = pd.concat([train['feature_38'], train['target']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['feature_38'], y=\"target\", data=data)\n#fig.axis(ymin=0, ymax=800000);\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:20:20.233154Z","iopub.execute_input":"2021-05-31T02:20:20.233546Z","iopub.status.idle":"2021-05-31T02:20:20.479891Z","shell.execute_reply.started":"2021-05-31T02:20:20.233515Z","shell.execute_reply":"2021-05-31T02:20:20.479389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"No outliers or missing values observed from EDA.","metadata":{}},{"cell_type":"markdown","source":"## Recombine datasets\n\nNo treatment is needed in this case.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\n\nObserved that the training set has large amount of zero values in the original 50 features, I have tried to add 50 binary features depending on whether each of the original 50 features is zero or not. The trial does not provide significant improvement in preliminary RF model, so this idea is not adopted. There maybe information loss in grouping all nonzero values together, which leads to worse performance.","metadata":{}},{"cell_type":"code","source":"'''\n# feature of zero or nonzero values\n\ndef zeroornot(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l] == 0).astype(int)) \n        res.columns.values[m] = l + '_zero'\n        m += 1\n    return res\n\ntrain_features_zero = zeroornot(train_features, train_features.columns.tolist())\ntest_features_zero = zeroornot(test_features, test_features.columns.tolist())\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.120317Z","iopub.execute_input":"2021-05-30T01:08:41.120727Z","iopub.status.idle":"2021-05-30T01:08:41.131284Z","shell.execute_reply.started":"2021-05-30T01:08:41.120693Z","shell.execute_reply":"2021-05-30T01:08:41.13009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_features_zero = train_features_zero.drop(train_features_zero.iloc[:,0:50], axis=1)\n#test_features_zero = test_features_zero.drop(test_features_zero.iloc[:,0:50], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.133534Z","iopub.execute_input":"2021-05-30T01:08:41.13397Z","iopub.status.idle":"2021-05-30T01:08:41.139867Z","shell.execute_reply.started":"2021-05-30T01:08:41.133932Z","shell.execute_reply":"2021-05-30T01:08:41.138964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA\n\nSince there are 50 features, the dimension reduction technique may help. I have tried PCA, but the result is not satisfactory. This is intuitive given the low features correlation shown in EDA, and the almost identical contributions from all the principal components.","metadata":{}},{"cell_type":"code","source":"'''\nX=train_features\n# Standardize\nX_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nX_pca.head()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:48:36.4042Z","iopub.execute_input":"2021-05-31T02:48:36.404493Z","iopub.status.idle":"2021-05-31T02:48:36.692635Z","shell.execute_reply.started":"2021-05-31T02:48:36.40447Z","shell.execute_reply":"2021-05-31T02:48:36.691866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:48:42.130461Z","iopub.execute_input":"2021-05-31T02:48:42.13075Z","iopub.status.idle":"2021-05-31T02:48:42.237051Z","shell.execute_reply.started":"2021-05-31T02:48:42.130724Z","shell.execute_reply":"2021-05-31T02:48:42.235929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 0.05)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\n# Look at explained variance\nplot_variance(pca);\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:49:43.268505Z","iopub.execute_input":"2021-05-31T02:49:43.2688Z","iopub.status.idle":"2021-05-31T02:49:43.600069Z","shell.execute_reply.started":"2021-05-31T02:49:43.268752Z","shell.execute_reply":"2021-05-31T02:49:43.599291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X_pca, train_labels, discrete_features=False)\nmi_scores\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:52:48.397205Z","iopub.execute_input":"2021-05-31T02:52:48.397485Z","iopub.status.idle":"2021-05-31T02:53:29.031739Z","shell.execute_reply.started":"2021-05-31T02:52:48.397453Z","shell.execute_reply":"2021-05-31T02:53:29.030727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nmiindex = mi_scores.index[mi_scores.values>0]\nmiload = loadings[miindex]\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:53:58.88637Z","iopub.execute_input":"2021-05-31T02:53:58.886646Z","iopub.status.idle":"2021-05-31T02:53:58.893099Z","shell.execute_reply.started":"2021-05-31T02:53:58.886623Z","shell.execute_reply":"2021-05-31T02:53:58.892251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntrain_features_pca = pd.DataFrame(data = np.matmul(train_features,miload))\ntrain_features_pca.columns=miindex\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:54:03.622058Z","iopub.execute_input":"2021-05-31T02:54:03.622521Z","iopub.status.idle":"2021-05-31T02:54:03.660151Z","shell.execute_reply.started":"2021-05-31T02:54:03.622481Z","shell.execute_reply":"2021-05-31T02:54:03.659295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntest_features_pca = pd.DataFrame(data = np.matmul(test_features,miload))\ntest_features_pca.columns=miindex\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.205278Z","iopub.execute_input":"2021-05-30T01:08:41.205734Z","iopub.status.idle":"2021-05-30T01:08:41.213873Z","shell.execute_reply.started":"2021-05-30T01:08:41.205697Z","shell.execute_reply":"2021-05-30T01:08:41.213011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recreate training and test sets\n\nNo treatment is needed in this case.","metadata":{}},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"# Model Validation and Selection","metadata":{}},{"cell_type":"markdown","source":"## Set up CV","metadata":{}},{"cell_type":"code","source":"# Set up cross validation folds\nkf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n# Define error metrics\ndef loss(y, y_pred):\n    return np.sqrt(log_loss(y, y_pred))\n\ndef cv_loss(model, X = train_features):\n    loss = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_log_loss\", cv=kf, n_jobs=-1))\n    return (loss)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.226511Z","iopub.execute_input":"2021-05-30T01:08:41.22693Z","iopub.status.idle":"2021-05-30T01:08:41.232893Z","shell.execute_reply.started":"2021-05-30T01:08:41.226848Z","shell.execute_reply":"2021-05-30T01:08:41.232007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Models","metadata":{}},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"\n# XGBoost Classifier\nxgb =  XGBClassifier(learning_rate = 0.1,\n                        colsample_bytree = 0.5,\n                        max_depth = 10,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.9,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n#xgb_model = xgb.fit(train_features, train_labels)\n#xgb_pred = xgb_model.predict_proba(test_features)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.247086Z","iopub.execute_input":"2021-05-30T01:08:41.247453Z","iopub.status.idle":"2021-05-30T01:08:41.257825Z","shell.execute_reply.started":"2021-05-30T01:08:41.24742Z","shell.execute_reply":"2021-05-30T01:08:41.257141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# XGBoost Classifier2\nxgb2 = XGBClassifier(n_estimators=110,\n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                       max_depth = 2,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n#xgb2_model = xgb2.fit(train_features, train_labels)\n#xgb2_pred = xgb2_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.258653Z","iopub.execute_input":"2021-05-30T01:08:41.259019Z","iopub.status.idle":"2021-05-30T01:08:41.268067Z","shell.execute_reply.started":"2021-05-30T01:08:41.25898Z","shell.execute_reply":"2021-05-30T01:08:41.267195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# XGBoost Classifier3\nxgb3 =XGBClassifier(n_estimators=250,\n                        learning_rate = 0.06,\n                        colsample_bytree = 0.4,\n                       max_depth = 6,\n                        min_child_weight=5,\n                       subsample=0.75,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       random_state=42)\n                       \n\n#xgb3_model = xgb3.fit(train_features, train_labels)\n#xgb3_pred = xgb3_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.269193Z","iopub.execute_input":"2021-05-30T01:08:41.26947Z","iopub.status.idle":"2021-05-30T01:08:41.279845Z","shell.execute_reply.started":"2021-05-30T01:08:41.269438Z","shell.execute_reply":"2021-05-30T01:08:41.279006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost Classifier4\nxgb4 = XGBClassifier(n_estimators=130, \n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                        max_depth = 2,\n                        min_child_weight=7, \n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                       \n\n#xgb4_model = xgb4.fit(train_features, train_labels)\n#xgb4_pred = xgb4_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.281083Z","iopub.execute_input":"2021-05-30T01:08:41.281384Z","iopub.status.idle":"2021-05-30T01:08:41.288687Z","shell.execute_reply.started":"2021-05-30T01:08:41.281356Z","shell.execute_reply":"2021-05-30T01:08:41.287816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost Classifier5\nxgb5 = XGBClassifier( n_estimators=200,\n                        learning_rate = 0.5,\n                       colsample_bytree = 0.1,\n                       max_depth = 2,\n                        min_child_weight=5,\n                      gamma=0.005,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=5,\n                         reg_lambda = 0.3,\n                         eval_metric = 'mlogloss',\n                       random_state=42)\n                       \n\n#xgb5_model = xgb5.fit(train_features, train_labels)\n#xgb5_pred = xgb5_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.28996Z","iopub.execute_input":"2021-05-30T01:08:41.290248Z","iopub.status.idle":"2021-05-30T01:08:41.297453Z","shell.execute_reply.started":"2021-05-30T01:08:41.290224Z","shell.execute_reply":"2021-05-30T01:08:41.296594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"\n# Random Forest Classifier\nrf = RandomForestClassifier(min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = None,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            criterion = \"entropy\",\n                            n_estimators=500,\n                            max_features = 12,\n                            random_state = 42)\n'''\nrf_model = rf.fit(train_features, train_labels)\nrf_pred = rf_model.predict_proba(test_features)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.311184Z","iopub.execute_input":"2021-05-30T01:08:41.311736Z","iopub.status.idle":"2021-05-30T01:08:41.319767Z","shell.execute_reply.started":"2021-05-30T01:08:41.311645Z","shell.execute_reply":"2021-05-30T01:08:41.318927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Light GBM","metadata":{}},{"cell_type":"code","source":"\n# Light Gradient Boosting Regressor\nlgb =  LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=220,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb_model = lgb.fit(train_features, train_labels)\nlgb_pred = lgb_model.predict_proba(test_features)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T03:37:17.83534Z","iopub.execute_input":"2021-05-31T03:37:17.835634Z","iopub.status.idle":"2021-05-31T03:37:23.085894Z","shell.execute_reply.started":"2021-05-31T03:37:17.835612Z","shell.execute_reply":"2021-05-31T03:37:23.085036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Light Gradient Boosting Regressor\nlgb2 =  LGBMClassifier(objective='multiclass', \n                       num_leaves=2,\n                    max_depth=1,\n                       learning_rate=0.5, \n                       n_estimators=550,\n                      max_bin=25, \n                       bagging_fraction=0.6,\n                       bagging_freq=8, \n                       bagging_seed=8,\n                      feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb2_model = lgb2.fit(train_features, train_labels)\n#lgb2_pred = lgb2_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.329437Z","iopub.execute_input":"2021-05-30T01:08:41.329819Z","iopub.status.idle":"2021-05-30T01:08:41.340525Z","shell.execute_reply.started":"2021-05-30T01:08:41.329791Z","shell.execute_reply":"2021-05-30T01:08:41.339515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Light Gradient Boosting Regressor\nlgb3 = LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=7, \n                    max_depth=4,\n                       learning_rate=0.066, \n                       n_estimators=200,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb3_model = lgb3.fit(train_features, train_labels)\n#lgb3_pred = lgb3_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.341678Z","iopub.execute_input":"2021-05-30T01:08:41.341963Z","iopub.status.idle":"2021-05-30T01:08:41.35034Z","shell.execute_reply.started":"2021-05-30T01:08:41.341938Z","shell.execute_reply":"2021-05-30T01:08:41.349549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Light Gradient Boosting Regressor\nlgb4 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=250, \n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.5,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb4_model = lgb4.fit(train_features, train_labels)\n#lgb4_pred = lgb4_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.351552Z","iopub.execute_input":"2021-05-30T01:08:41.351825Z","iopub.status.idle":"2021-05-30T01:08:41.362419Z","shell.execute_reply.started":"2021-05-30T01:08:41.351799Z","shell.execute_reply":"2021-05-30T01:08:41.361542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Light Gradient Boosting Regressor\nlgb5 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=6,\n                       learning_rate=0.1, \n                      n_estimators=310, \n                       max_bin=80, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb5_model = lgb5.fit(train_features, train_labels)\n#lgb5_pred = lgb5_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.363698Z","iopub.execute_input":"2021-05-30T01:08:41.364064Z","iopub.status.idle":"2021-05-30T01:08:41.371991Z","shell.execute_reply.started":"2021-05-30T01:08:41.364035Z","shell.execute_reply":"2021-05-30T01:08:41.37111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Light Gradient Boosting Regressor\nlgb6 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=5,\n                      learning_rate=0.1, \n                      n_estimators=310, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                           lambda_l2 = 0.003,\n                           min_gain_to_split = 0.0001,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb6_model = lgb6.fit(train_features, train_labels)\n#lgb6_pred = lgb6_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.373378Z","iopub.execute_input":"2021-05-30T01:08:41.373724Z","iopub.status.idle":"2021-05-30T01:08:41.383618Z","shell.execute_reply.started":"2021-05-30T01:08:41.373697Z","shell.execute_reply":"2021-05-30T01:08:41.382774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Light Gradient Boosting Regressor\nlgb7 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=5,\n                      learning_rate=0.1, \n                     n_estimators=500, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 9,\n                           lambda_l2 = 0.003,\n                           min_gain_to_split = 0.0001,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb7_model = lgb7.fit(train_features, train_labels)\n#lgb7_pred = lgb7_model.predict_proba(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.384761Z","iopub.execute_input":"2021-05-30T01:08:41.385006Z","iopub.status.idle":"2021-05-30T01:08:41.392319Z","shell.execute_reply.started":"2021-05-30T01:08:41.384983Z","shell.execute_reply":"2021-05-30T01:08:41.391622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extra Trees","metadata":{}},{"cell_type":"code","source":"\n# Extra Trees Classifier\next =ExtraTreesClassifier(  min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = 15,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            n_estimators=10,\n                            max_features = 20,\n                            random_state = 42,\n                            criterion = 'entropy')\n'''\next_model = ext.fit(train_features, train_labels)\next_pred = ext_model.predict_proba(test_features)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.39354Z","iopub.execute_input":"2021-05-30T01:08:41.393787Z","iopub.status.idle":"2021-05-30T01:08:41.406205Z","shell.execute_reply.started":"2021-05-30T01:08:41.393764Z","shell.execute_reply":"2021-05-30T01:08:41.405403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost","metadata":{}},{"cell_type":"code","source":"\n#CatBoost\ncat_features = train_features.columns.values.tolist()\n\ntrain_dataset = Pool(data=train_features,\n                     label=train_labels,\n                     cat_features=cat_features)\n\neval_dataset = Pool(data=test_features,\n                    cat_features=cat_features)\n\n# Initialize CatBoostClassifier\ncat =  CatBoostClassifier(n_estimators=500,\n                           learning_rate=0.3,\n                           max_depth=2,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5,\n                            verbose=0)\n# Fit model\n#cat.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat_pred = cat.predict_proba(eval_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:41.407589Z","iopub.execute_input":"2021-05-30T01:08:41.407907Z","iopub.status.idle":"2021-05-30T01:08:44.270676Z","shell.execute_reply.started":"2021-05-30T01:08:41.407882Z","shell.execute_reply":"2021-05-30T01:08:44.269808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#CatBoost2\n\n\n# Initialize CatBoostClassifier\ncat2 = CatBoostClassifier(n_estimators=550,\n                           learning_rate=0.5,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            verbose=0)\n# Fit model\n#cat2.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat2_pred = cat2.predict_proba(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.271743Z","iopub.execute_input":"2021-05-30T01:08:44.271997Z","iopub.status.idle":"2021-05-30T01:08:44.277333Z","shell.execute_reply.started":"2021-05-30T01:08:44.271973Z","shell.execute_reply":"2021-05-30T01:08:44.276263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#CatBoost3\n\n\n# Initialize CatBoostClassifier\ncat3 = CatBoostClassifier(n_estimators=250,\n                           learning_rate=0.1,\n                           max_depth=6,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=3,\n                            verbose=0)\n\n# Fit model\n#cat3.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat3_pred = cat3.predict_proba(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.278505Z","iopub.execute_input":"2021-05-30T01:08:44.27883Z","iopub.status.idle":"2021-05-30T01:08:44.286735Z","shell.execute_reply.started":"2021-05-30T01:08:44.278805Z","shell.execute_reply":"2021-05-30T01:08:44.286043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CatBoost4\n\n# Initialize CatBoostClassifier\n\ncat4 = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            verbose=0)\n\n\n# Fit model\n#cat4.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat4_pred = cat4.predict_proba(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.287929Z","iopub.execute_input":"2021-05-30T01:08:44.288454Z","iopub.status.idle":"2021-05-30T01:08:44.296002Z","shell.execute_reply.started":"2021-05-30T01:08:44.288417Z","shell.execute_reply":"2021-05-30T01:08:44.295274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CatBoost5\n\n# Initialize CatBoostClassifier\n\ncat5 = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                               bagging_temperature = 0.2,\n                             sampling_frequency = 'PerTreeLevel',\n                               reg_lambda = 3,\n                            verbose=0)\n\n# Fit model\n#cat5.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat5_pred = cat5.predict_proba(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.296976Z","iopub.execute_input":"2021-05-30T01:08:44.297438Z","iopub.status.idle":"2021-05-30T01:08:44.309798Z","shell.execute_reply.started":"2021-05-30T01:08:44.2974Z","shell.execute_reply":"2021-05-30T01:08:44.309113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking\n\nOnly one level of meta model is trained on the base model predictions and the original features. The type of meta models would still be chosen from the available types in the base models. ","metadata":{}},{"cell_type":"code","source":"%%time\n# Stack up all the models above, optimized using lgb\nstack_gen = StackingCVClassifier(classifiers = (rf, lgb, xgb, ext),\n                                meta_classifier = lgb,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17)\n\n#stack_gen_model = stack_gen.fit(np.array(train_features), np.array(train_labels))\n#stack_pred = stack_gen_model.predict_proba(np.array(test_features))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.311037Z","iopub.execute_input":"2021-05-30T01:08:44.311547Z","iopub.status.idle":"2021-05-30T01:08:44.320329Z","shell.execute_reply.started":"2021-05-30T01:08:44.311508Z","shell.execute_reply":"2021-05-30T01:08:44.31951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n%%time\n# Stack2\n# Stack up all the models above, optimized using cat\nstack_gen2 = StackingCVClassifier(classifiers = (xgb2, lgb, cat),\n                                meta_classifier = cat,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17)\n\n#stack_gen2_model = stack_gen2.fit(np.array(train_features), np.array(train_labels))\n#stack2_pred = stack_gen2_model.predict_proba(np.array(test_features))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.321555Z","iopub.execute_input":"2021-05-30T01:08:44.32199Z","iopub.status.idle":"2021-05-30T01:08:44.329828Z","shell.execute_reply.started":"2021-05-30T01:08:44.321963Z","shell.execute_reply":"2021-05-30T01:08:44.328894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack4\n# Stack up all the models above, optimized using cat2\nstack_gen4 = StackingCVClassifier(classifiers = (xgb2,lgb, cat2),\n                                meta_classifier = cat,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17\n                                )\n\n#stack_gen4_model = stack_gen4.fit(np.array(train_features), np.array(train_labels))\n#stack4_pred = stack_gen4_model.predict_proba(np.array(test_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.330907Z","iopub.execute_input":"2021-05-30T01:08:44.331441Z","iopub.status.idle":"2021-05-30T01:08:44.342653Z","shell.execute_reply.started":"2021-05-30T01:08:44.331366Z","shell.execute_reply":"2021-05-30T01:08:44.341742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack5\n# Stack up all the models above, optimized using cats5\n\ncats5 = CatBoostClassifier(n_estimators=450, \n                           learning_rate=0.1, \n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5,\n                            verbose=0)\n                            \n\nstack_gen5 = StackingCVClassifier(classifiers = (xgb2,lgb, cat2),\n                                meta_classifier = cats5,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n#stack_gen5_model = stack_gen5.fit(np.array(train_features), np.array(train_labels))\n#stack5_pred = stack_gen5_model.predict_proba(np.array(test_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.3439Z","iopub.execute_input":"2021-05-30T01:08:44.344275Z","iopub.status.idle":"2021-05-30T01:08:44.353626Z","shell.execute_reply.started":"2021-05-30T01:08:44.344246Z","shell.execute_reply":"2021-05-30T01:08:44.352569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack6\n# Stack up all the models above, optimized using lgbs6\n\nlgbs6 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.03, \n                       n_estimators=160, \n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                            \n\nstack_gen6 = StackingCVClassifier(classifiers = (lgb, xgb, cat),\n                                meta_classifier = lgbs6,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n#stack_gen6_model = stack_gen6.fit(np.array(train_features), np.array(train_labels))\n#stack6_pred = stack_gen6_model.predict_proba(np.array(test_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.354572Z","iopub.execute_input":"2021-05-30T01:08:44.354937Z","iopub.status.idle":"2021-05-30T01:08:44.36839Z","shell.execute_reply.started":"2021-05-30T01:08:44.35491Z","shell.execute_reply":"2021-05-30T01:08:44.367541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack7\n# Stack up all the models above, optimized using lgbs7\n\n\nlgbs7 = LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=7, \n                    max_depth=4,\n                       learning_rate=0.05, \n                       n_estimators=100,\n                       bagging_fraction=1,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nstack_gen7 =  StackingCVClassifier(classifiers = (lgb3, xgb3, cat),\n                                meta_classifier = lgbs7,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n#stack_gen7_model = stack_gen7.fit(np.array(train_features), np.array(train_labels))\n#stack7_pred = stack_gen7_model.predict_proba(np.array(test_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.369952Z","iopub.execute_input":"2021-05-30T01:08:44.370407Z","iopub.status.idle":"2021-05-30T01:08:44.382476Z","shell.execute_reply.started":"2021-05-30T01:08:44.370346Z","shell.execute_reply":"2021-05-30T01:08:44.38146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack8\n# Stack up all the models above, optimized using lgbs8\n\n\nlgbs8 = LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=3, \n                    max_depth=4,\n                       learning_rate=0.05, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nstack_gen8 = StackingCVClassifier(classifiers = (xgb2, lgb, cat2),\n                                meta_classifier = lgbs8,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n#stack_gen8_model = stack_gen8.fit(np.array(train_features), np.array(train_labels))\n#stack8_pred = stack_gen8_model.predict_proba(np.array(test_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.384033Z","iopub.execute_input":"2021-05-30T01:08:44.384491Z","iopub.status.idle":"2021-05-30T01:08:44.396723Z","shell.execute_reply.started":"2021-05-30T01:08:44.384453Z","shell.execute_reply":"2021-05-30T01:08:44.395822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack9\n# Stack up all the models above, optimized using lgbs9\n\n\nlgbs9 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                   min_data_in_leaf=23, \n                    max_depth=3,\n                       learning_rate=0.05, \n                    n_estimators=270, \n                      max_bin=60, \n                    bagging_fraction=0.7, \n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nstack_gen9 = StackingCVClassifier(classifiers = (xgb4, lgb5, cat4),\n                                meta_classifier = lgbs9,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n#stack_gen9_model = stack_gen9.fit(np.array(train_features), np.array(train_labels))\n#stack9_pred = stack_gen9_model.predict_proba(np.array(test_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.397941Z","iopub.execute_input":"2021-05-30T01:08:44.398247Z","iopub.status.idle":"2021-05-30T01:08:44.410344Z","shell.execute_reply.started":"2021-05-30T01:08:44.398221Z","shell.execute_reply":"2021-05-30T01:08:44.40937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack10\n# Stack up all the models above, optimized using lgbs10\n\n\nlgbs10 = LGBMClassifier(objective='multiclass', \n                       num_leaves=4,\n                  min_data_in_leaf=400, \n                    max_depth=1, \n                     learning_rate=0.05, \n                   n_estimators=270, \n                      max_bin=60, \n                    bagging_fraction=0.5, \n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.15, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n                            \n\nstack_gen10 = StackingCVClassifier(classifiers = (xgb4, lgb5, cat4),\n                                meta_classifier = lgbs10,\n                                 use_probas= True,\n                                use_features_in_secondary= False, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n#stack_gen10_model = stack_gen10.fit(np.array(train_features), np.array(train_labels))\n#stack10_pred = stack_gen10_model.predict_proba(np.array(test_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.411766Z","iopub.execute_input":"2021-05-30T01:08:44.412088Z","iopub.status.idle":"2021-05-30T01:08:44.42604Z","shell.execute_reply.started":"2021-05-30T01:08:44.412061Z","shell.execute_reply":"2021-05-30T01:08:44.425185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack11\n# Stack up all the models above, optimized using lgbs11\n\n\nlgbs11 = LGBMClassifier(objective='multiclass', \n                       num_leaves=5,\n                    min_data_in_leaf=19, \n                    max_depth=4,\n                      learning_rate=0.1, \n                     n_estimators=180, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 7,\n                           lambda_l2 = 0.1,\n                           min_gain_to_split = 0.01,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                       \nstack_gen11 = StackingCVClassifier(classifiers = (xgb5, lgb7, cat5),\n                                meta_classifier = lgbs11,\n                                 use_probas= True,\n                                use_features_in_secondary= True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\nstack_gen11_model = stack_gen11.fit(np.array(train_features), np.array(train_labels))\nstack11_pred = stack_gen11_model.predict_proba(np.array(test_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T07:24:27.367229Z","iopub.execute_input":"2021-05-30T07:24:27.367649Z","iopub.status.idle":"2021-05-30T07:24:27.44568Z","shell.execute_reply.started":"2021-05-30T07:24:27.367557Z","shell.execute_reply":"2021-05-30T07:24:27.444353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV result","metadata":{}},{"cell_type":"code","source":"\nscores = {}\n\nscore = cv_loss(stack_gen11)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.478715Z","iopub.execute_input":"2021-05-30T01:08:44.479096Z","iopub.status.idle":"2021-05-30T01:08:44.491569Z","shell.execute_reply.started":"2021-05-30T01:08:44.479069Z","shell.execute_reply":"2021-05-30T01:08:44.490327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Redefine CV function for PCA","metadata":{}},{"cell_type":"code","source":"'''\ndef cvpca_loss(model, X = train_features_pca):\n    loss = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_log_loss\", cv=kf, n_jobs=-1))\n    return (loss)\n\nscores = {}\n\nscore = cvpca_loss(cat2)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.49322Z","iopub.execute_input":"2021-05-30T01:08:44.493668Z","iopub.status.idle":"2021-05-30T01:08:44.499916Z","shell.execute_reply.started":"2021-05-30T01:08:44.493617Z","shell.execute_reply":"2021-05-30T01:08:44.499254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix\n\nThe result shows that the imbalanced target data has led to prediction highly in favour of \"Class 2\", which is worth noting for insights.","metadata":{}},{"cell_type":"code","source":"lgb_pred = lgb_model.predict(np.array(train_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T03:37:46.570245Z","iopub.execute_input":"2021-05-31T03:37:46.57051Z","iopub.status.idle":"2021-05-31T03:37:47.450451Z","shell.execute_reply.started":"2021-05-31T03:37:46.570487Z","shell.execute_reply":"2021-05-31T03:37:47.449777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(lgb_pred, return_counts=True)\nnp.asarray((unique, counts)).T","metadata":{"execution":{"iopub.status.busy":"2021-05-31T03:37:47.451474Z","iopub.execute_input":"2021-05-31T03:37:47.451698Z","iopub.status.idle":"2021-05-31T03:37:47.499581Z","shell.execute_reply.started":"2021-05-31T03:37:47.451675Z","shell.execute_reply":"2021-05-31T03:37:47.49889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.around(confusion_matrix(train_labels,lgb_pred, normalize = 'true'),3)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T03:38:33.630844Z","iopub.execute_input":"2021-05-31T03:38:33.631121Z","iopub.status.idle":"2021-05-31T03:38:33.977628Z","shell.execute_reply.started":"2021-05-31T03:38:33.631099Z","shell.execute_reply":"2021-05-31T03:38:33.976639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"## Validation Curves\n\nValidation curve is most frequently used in hyperparameters tuning for its lower computation cost. Past studies have also shown that its performance is not far from the grid search if used properly.","metadata":{}},{"cell_type":"code","source":"'''\n%%time\nnum_est = [100,200,300]\n\nvc_model = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                           # min_data_in_leaf=5,\n                               bagging_temperature = 0.2,\n                             sampling_frequency = 'PerTreeLevel',\n                               reg_lambda = 3,\n                            verbose=0)\n\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 5-fold cross validation\ntrain_score, test_score = validation_curve( vc_model,\n                                X = train_features, y = train_labels, \n                                param_name = 'min_data_in_leaf', \n                                param_range = num_est, cv = kf, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.540621Z","iopub.execute_input":"2021-05-30T01:08:44.540975Z","iopub.status.idle":"2021-05-30T01:08:44.550703Z","shell.execute_reply.started":"2021-05-30T01:08:44.540925Z","shell.execute_reply":"2021-05-30T01:08:44.549748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean_test_score","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.552005Z","iopub.execute_input":"2021-05-30T01:08:44.552364Z","iopub.status.idle":"2021-05-30T01:08:44.561093Z","shell.execute_reply.started":"2021-05-30T01:08:44.552338Z","shell.execute_reply":"2021-05-30T01:08:44.560196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Hyperparameter Grid\n\nThe random grid is mostly used as a preliminary to narrow down the range of hyperparameters for the finer grid search.","metadata":{}},{"cell_type":"code","source":"'''\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 10)]\n# Number of features to consider at every split\nmax_features = [int(x) for x in np.linspace(5, 15, num = 10)]\nmax_features.append(\"auto\")\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 10, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = 5\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = 5\n# Method of selecting samples for training each tree\nbootstrap = True\n\n#XGB\nlearning_rate = [0.01,0.1,0.3,0.5, 0.7,1]\ncolsample_bytree = [0.05,0.1, 0.3, 0.5,0.7,1]\n\n#LGB\nfeature_fraction = [0.1, 0.3, 0.5, 0.7, 0.9, 1]\n\n#CAT\ncolsample_bylevel =[0.1, 0.3, 0.5, 0.7, 0.9, 1]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               #'max_features': max_features,\n               'max_depth': max_depth,\n               #'min_samples_split': min_samples_split,\n               #'min_samples_leaf': min_samples_leaf,\n               #'bootstrap': bootstrap,\n               'learning_rate': learning_rate,\n               #'colsample_bytree': colsample_bytree\n               #'feature_fraction':feature_fraction,\n               'colsample_bylevel':colsample_bylevel\n              }\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.567255Z","iopub.execute_input":"2021-05-30T01:08:44.56751Z","iopub.status.idle":"2021-05-30T01:08:44.573723Z","shell.execute_reply.started":"2021-05-30T01:08:44.567486Z","shell.execute_reply":"2021-05-30T01:08:44.57306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = CatBoostClassifier(#n_estimators=550,\n                           #learning_rate=0.5,\n                           #max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            #colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            reg_lambda = 3,\n                            subsample = 0.8,\n                            bootstrap_type = 'Bernoulli'\n\n                            )\n# Random search of parameters, using 2 fold cross validation, \n# search across 30 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 33, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_features, train_labels)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.575241Z","iopub.execute_input":"2021-05-30T01:08:44.575493Z","iopub.status.idle":"2021-05-30T01:08:44.585541Z","shell.execute_reply.started":"2021-05-30T01:08:44.575469Z","shell.execute_reply":"2021-05-30T01:08:44.584597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf_random.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.586767Z","iopub.execute_input":"2021-05-30T01:08:44.587192Z","iopub.status.idle":"2021-05-30T01:08:44.595238Z","shell.execute_reply.started":"2021-05-30T01:08:44.58713Z","shell.execute_reply":"2021-05-30T01:08:44.594289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grid Search","metadata":{}},{"cell_type":"code","source":"'''\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    #'max_depth': [5,7],\n    'n_estimators': [220,240,260],\n    #'learning_rate':[0.1,0.3,0.5],\n    #'feature_fraction':[0.1,0.3],\n    #'num_leaves':[2,6,10],\n    #'bagging_fraction':[0.4,0.6,0.8],\n    #'bagging_freq':[6,8,10],\n    #'max_bin':[10,15,25],\n    #'colsample_bylevel':[0.05,0.1,0.3],\n    'min_data_in_leaf':[3,5,7],\n    'reg_lambda':[1,3,5],\n    'subsample':[0.6,0.8,1]\n    \n}\n# Create a based model\nrf = XGBClassifier(n_estimators=100,\n                        learning_rate = 0.5,\n                        colsample_bytree = 0.4,\n                       max_depth = 6,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.59638Z","iopub.execute_input":"2021-05-30T01:08:44.596638Z","iopub.status.idle":"2021-05-30T01:08:44.605913Z","shell.execute_reply.started":"2021-05-30T01:08:44.596615Z","shell.execute_reply":"2021-05-30T01:08:44.605257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Fit the grid search to the data\ngrid_search.fit(train_features, train_labels)\n#best_grid = \ngrid_search.best_estimator_\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.607049Z","iopub.execute_input":"2021-05-30T01:08:44.607623Z","iopub.status.idle":"2021-05-30T01:08:44.617845Z","shell.execute_reply.started":"2021-05-30T01:08:44.607571Z","shell.execute_reply":"2021-05-30T01:08:44.616867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.619772Z","iopub.execute_input":"2021-05-30T01:08:44.620227Z","iopub.status.idle":"2021-05-30T01:08:44.626359Z","shell.execute_reply.started":"2021-05-30T01:08:44.620198Z","shell.execute_reply":"2021-05-30T01:08:44.625599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning for meta-model","metadata":{}},{"cell_type":"code","source":"#stack_gen.get_params().keys()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T01:08:44.627444Z","iopub.execute_input":"2021-05-30T01:08:44.627722Z","iopub.status.idle":"2021-05-30T01:08:44.636332Z","shell.execute_reply.started":"2021-05-30T01:08:44.627699Z","shell.execute_reply":"2021-05-30T01:08:44.635497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n%%time\nnum_est = [0.08,0.1,0.12]\n\nvc_model = LGBMClassifier(objective='multiclass', \n                       num_leaves=5,\n                    min_data_in_leaf=19, \n                    max_depth=4,\n                      learning_rate=0.1, \n                     n_estimators=180, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 7,\n                           lambda_l2 = 0.1,\n                           min_gain_to_split = 0.01,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                       \nstack_gen_vc = StackingCVClassifier(classifiers = (xgb5, lgb7, cat5),\n                                meta_classifier = vc_model,\n                                 use_probas= True,\n                                use_features_in_secondary= True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 3-fold cross validation\ntrain_score, test_score = validation_curve( stack_gen_vc,\n                                X = train_features, y = train_labels, \n                                param_name = 'meta_classifier__learning_rate', \n                                param_range = num_est, cv = kf, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:05:37.895404Z","iopub.execute_input":"2021-05-30T04:05:37.895718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean_test_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"\n# Read in sample_submission dataframe\nsubmission[['Class_1', 'Class_2', 'Class_3', 'Class_4']] = stack11_pred\nsubmission.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.652476Z","iopub.execute_input":"2021-05-23T06:59:03.653215Z","iopub.status.idle":"2021-05-23T06:59:03.664749Z","shell.execute_reply.started":"2021-05-23T06:59:03.653162Z","shell.execute_reply":"2021-05-23T06:59:03.663507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission_stack11.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.666844Z","iopub.execute_input":"2021-05-23T06:59:03.66743Z","iopub.status.idle":"2021-05-23T06:59:03.677301Z","shell.execute_reply.started":"2021-05-23T06:59:03.667388Z","shell.execute_reply":"2021-05-23T06:59:03.676135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance Log","metadata":{}},{"cell_type":"code","source":"'''\nPerformance\n# CV scores have taken square root by careless setting, but would not affect the comparison of results.\n\nBasic rf (n=100)\nCV 1.0633 (0.0022)\npublic 1.12521\n\nFeatured rf (n=100)\nCV 1.0643 (0.0024)\npublic 1.12140\n\nTuned rf\nCV 1.0506 (0.0005)\npublic 1.09957\n\nTuned rf with criterion = \"entropy\"\nCV 1.0501 (0.0007)\npublic 1.09912\n\nBasic xgb\npublic 1.09495\n\nTuned xgb\nCV 1.0473 (0.0007)\npublic 1.08944\n\nTuned2 xgb\nCV 1.0453 (0.0006)\npublic 1.08896\n\nTuned3 xgb\nCV 1.0455 (0.0005)\npublic 1.08767\n\nTuned4 xgb\nCV 1.0453 (0.0007)\npublic 1.08834\n\nTuned5 xgb\nCV 1.0445 (0.0007)\npublic 1.08785\n\nBasic lgb\nCV 1.0470 (0.0006)\npublic 1.09036\n\nTuned lgb\nCV 1.0454 (0.0007)\npublic 1.08871\n\nTuned2 lgb\nCV 1.0459 (0.0007)\npublic 1.08916\n\nTuned3 lgb\nCV 1.0455 (0.0007)\npublic 1.08872\n\nTuned4 lgb\nCV 1.0452 (0.0007)\npublic 1.08789\n\nTuned5 lgb\nCV 1.0447 (0.0006)\npublic 1.08785\n\nTuned6 lgb\nCV 1.0447 (0.0006)\npublic 1.08790\n\nTuned7 lgb\nCV 1.0441 (0.0008)\npublic 1.08723\n\nBasic ext\nCV 1.0643 (0.0011)\npublic 1.12962\n\nTuned ext\nCV 1.0550 (0.0004)\npublic 1.10729\n\nTuned2 ext\nCV 1.0536 (0.0005)\npublic 1.10495\n\nBasic cat\nCV 1.0516 (0.0009)\npublic 1.10367\n\nTuned cat\nCV 1.0450 (0.0008)\npublic 1.09039\n\nTuned2 cat\nCV 1.0449 (0.0007)\npublic 1.09092\n\nTuned3 cat\nCV 1.0456 (0.0006)\npublic 1.09146\n\nTuned4 cat\nCV 1.0448 (0.0009)\npublic 1.09089\n\nTuned5 cat\nCV 1.0446 (0.0007)\npublic 1.09073\n\nStacking (rf, xgb, lgb, ext)>lgb\nCV 1.0455 (0.0007)\npublic 1.08811\n\nStacking1 (xgb, lgb, cat)>lgb\nCV 1.0450 (0.0008)\npublic 1.08605\n\nStacking2 (xgb2, lgb, cat)>cat\nCV 1.0448 (0.0008)\npublic 1.08766\n\nStacking3 (xgb2, lgb, cat)>xgb2\nCV 1.0458 (0.0008)\npublic 1.08729\n\nStacking4 (xgb2, lgb, cat2)>cat\nCV 1.0447 (0.0007)\npublic 1.08679\n\nStacking5 (xgb2, lgb, cat2)>cats5\nCV 1.0443 (0.0008)\npublic 1.08685\n\nStacking6 (xgb, lgb, cat)>lgbs6\nCV 1.0443 (0.0007)\npublic 1.08604\n\nStacking7 (xgb3, lgb3, cat)>lgbs7\nCV 1.0443 (0.0008)\npublic 1.08641\n\nStacking8 (xgb2, lgb, cat2)>lgbs8\nCV 1.0444 (0.0007)\npublic 1.08592\n\nStacking9 (xgb4, lgb5, cat4)>lgbs9\nCV 1.0439 (0.0008)\npublic 1.08613\n\nStacking10 (Stacking9 with no features input to meta model)\nCV 1.0440 (0.0007)\npublic 1.08674\n\nStacking11 (xgb5, lgb7, cat5)>lgbs11\nCV 1.0436 (0.0008)\npublic 1.08627\n\n\nrf = RandomForestClassifier(min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = None,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            criterion = \"entropy\",\n                            n_estimators=500,\n                            max_features = 12,\n                            random_state = 42)\n\nxgb1 = XGBClassifier(learning_rate = 0.1,\n                        colsample_bytree = 0.5,\n                        max_depth = 10,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.9,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                       \nxgb2 = XGBClassifier(n_estimators=110,\n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                       max_depth = 2,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                       \n                       \nxgb3 = XGBClassifier(n_estimators=250,\n                        learning_rate = 0.06,\n                        colsample_bytree = 0.4,\n                       max_depth = 6,\n                        min_child_weight=5,\n                       subsample=0.75,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       random_state=42)\n\nxgb4 = XGBClassifier(n_estimators=130, \n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                        max_depth = 2,\n                        min_child_weight=7, \n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\nxgb5 = XGBClassifier( n_estimators=200,\n                        learning_rate = 0.5,\n                       colsample_bytree = 0.1,\n                       max_depth = 2,\n                        min_child_weight=5,\n                      gamma=0.005,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=5,\n                         reg_lambda = 0.3,\n                         eval_metric = 'mlogloss',\n                       random_state=42)\n                       \nlgb = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=220,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgb2 = LGBMClassifier(objective='multiclass', \n                       num_leaves=2,\n                    max_depth=1,\n                       learning_rate=0.5, \n                       n_estimators=550,\n                      max_bin=25, \n                       bagging_fraction=0.6,\n                       bagging_freq=8, \n                       bagging_seed=8,\n                      feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgb3 =  LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=7, \n                    max_depth=4,\n                       learning_rate=0.066, \n                       n_estimators=200,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb4 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=250, \n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.5,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb5 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=6,\n                       learning_rate=0.1, \n                      n_estimators=310, \n                       max_bin=80, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb6 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=5,\n                      learning_rate=0.1, \n                      n_estimators=310, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                           lambda_l2 = 0.003,\n                           min_gain_to_split = 0.0001,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb7 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=5,\n                      learning_rate=0.1, \n                     n_estimators=500, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 9,\n                           lambda_l2 = 0.003,\n                           min_gain_to_split = 0.0001,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n                   \next = ExtraTreesClassifier(  min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = 15,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            n_estimators=10,\n                            max_features = 20,\n                            random_state = 42,\n                            criterion = 'entropy')\n                            \n\ncat = CatBoostClassifier(n_estimators=500,\n                           learning_rate=0.3,\n                           max_depth=2,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5,\n                            verbose=0)\n                            \n                            \ncat2 = CatBoostClassifier(n_estimators=550,\n                           learning_rate=0.5,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            verbose=0)\n                            \n                            \ncat3 =   CatBoostClassifier(n_estimators=250,\n                           learning_rate=0.1,\n                           max_depth=6,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=3,\n                            verbose=0)\n\ncat4 = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            verbose=0)\n\ncat5 = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                               bagging_temperature = 0.2,\n                             sampling_frequency = 'PerTreeLevel',\n                               reg_lambda = 3,\n                            verbose=0)\n\n\ncats5 = CatBoostClassifier(n_estimators=450, \n                           learning_rate=0.1, \n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5,\n                            verbose=0)\n                            \n                            \nlgbs6 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.03, \n                       n_estimators=160, \n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \n                   \nlgbs7 = LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=7, \n                    max_depth=4,\n                       learning_rate=0.05, \n                       n_estimators=100,\n                       bagging_fraction=1,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgbs8 = LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=3, \n                    max_depth=4,\n                       learning_rate=0.05, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgbs9 =  LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                   min_data_in_leaf=23, \n                    max_depth=3,\n                       learning_rate=0.05, \n                    n_estimators=270, \n                      max_bin=60, \n                    bagging_fraction=0.7, \n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgbs10 = LGBMClassifier(objective='multiclass', \n                       num_leaves=4,\n                  min_data_in_leaf=400, \n                    max_depth=1, \n                     learning_rate=0.05, \n                   n_estimators=270, \n                      max_bin=60, \n                    bagging_fraction=0.5, \n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.15, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgbs11 = LGBMClassifier(objective='multiclass', \n                       num_leaves=5,\n                    min_data_in_leaf=19, \n                    max_depth=4,\n                      learning_rate=0.1, \n                     n_estimators=180, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 7,\n                           lambda_l2 = 0.1,\n                           min_gain_to_split = 0.01,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.679461Z","iopub.execute_input":"2021-05-23T06:59:03.680061Z","iopub.status.idle":"2021-05-23T06:59:03.698299Z","shell.execute_reply.started":"2021-05-23T06:59:03.680009Z","shell.execute_reply":"2021-05-23T06:59:03.696992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional: Class 2 pre-modelling","metadata":{}},{"cell_type":"markdown","source":"Try to do modelling in sequence that first trained on separating \"Class 2\" from others, and then classify Class 1, 3, 4 in the remaining block. Result not satisfactory.","metadata":{}},{"cell_type":"code","source":"'''\n# Split features and labels\ntrain2_labels = train_labels.apply(lambda x: 'Class_2'  if x == 'Class_2' else 'Others')\ntrain2_features = train_features\ntrain2_labels.unique()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.699817Z","iopub.execute_input":"2021-05-23T06:59:03.700219Z","iopub.status.idle":"2021-05-23T06:59:03.715425Z","shell.execute_reply.started":"2021-05-23T06:59:03.700177Z","shell.execute_reply":"2021-05-23T06:59:03.714536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntrain134_labels = train.loc[train['target']!='Class_2']['target'].reset_index(drop=True)\ntrain134_features = train.loc[train['target']!='Class_2'].drop(['id','target'], axis=1).reset_index(drop=True)\ntrain134_labels.unique()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.716636Z","iopub.execute_input":"2021-05-23T06:59:03.717126Z","iopub.status.idle":"2021-05-23T06:59:03.727524Z","shell.execute_reply.started":"2021-05-23T06:59:03.717061Z","shell.execute_reply":"2021-05-23T06:59:03.72669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Light Gradient Boosting Regressor\nlgbp2 =  LGBMClassifier(objective='binary', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.066, \n                       n_estimators=250,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgbp2_model = lgbp2.fit(train2_features, train2_labels)\n#lgbp2_pred = lgbp2_model.predict_proba(test_features)\n\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.728638Z","iopub.execute_input":"2021-05-23T06:59:03.729117Z","iopub.status.idle":"2021-05-23T06:59:03.746198Z","shell.execute_reply.started":"2021-05-23T06:59:03.729054Z","shell.execute_reply":"2021-05-23T06:59:03.745048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Light Gradient Boosting Regressor\nlgbp134 =  LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.1, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.4,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n\n#lgbp134_model = lgbp134.fit(train134_features, train134_labels)\n#lgbp134_pred = lgbp134_model.predict_proba(test_features)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.747651Z","iopub.execute_input":"2021-05-23T06:59:03.747981Z","iopub.status.idle":"2021-05-23T06:59:03.759315Z","shell.execute_reply.started":"2021-05-23T06:59:03.747946Z","shell.execute_reply":"2021-05-23T06:59:03.75827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n%%time\nnum_est = [0.3,0.4,0.5]\n\nvc_model = LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.1, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.4,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 3-fold cross validation\ntrain_score, test_score = validation_curve( vc_model,\n                                X = train134_features, y = train134_labels, \n                                param_name = 'feature_fraction', \n                                param_range = num_est, cv = 2, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.76078Z","iopub.execute_input":"2021-05-23T06:59:03.761289Z","iopub.status.idle":"2021-05-23T06:59:03.77416Z","shell.execute_reply.started":"2021-05-23T06:59:03.761237Z","shell.execute_reply":"2021-05-23T06:59:03.772925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean_test_score","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.775797Z","iopub.execute_input":"2021-05-23T06:59:03.776239Z","iopub.status.idle":"2021-05-23T06:59:03.790479Z","shell.execute_reply.started":"2021-05-23T06:59:03.776185Z","shell.execute_reply":"2021-05-23T06:59:03.789342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Read in sample_submission dataframe\nsubmission[['Class_1']] = lgbp2_pred[:,1]*lgbp134_pred[:,0]\nsubmission[['Class_2']] =lgbp2_pred[:,0]\nsubmission[['Class_3']] = lgbp2_pred[:,1]*lgbp134_pred[:,1]\nsubmission[['Class_4']] = lgbp2_pred[:,1]*lgbp134_pred[:,2]\n\nsubmission.head()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.792147Z","iopub.execute_input":"2021-05-23T06:59:03.792729Z","iopub.status.idle":"2021-05-23T06:59:03.806427Z","shell.execute_reply.started":"2021-05-23T06:59:03.792691Z","shell.execute_reply":"2021-05-23T06:59:03.805495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission[['Class_1','Class_2','Class_3','Class_4']].sum(axis=1).unique()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.80781Z","iopub.execute_input":"2021-05-23T06:59:03.808326Z","iopub.status.idle":"2021-05-23T06:59:03.819131Z","shell.execute_reply.started":"2021-05-23T06:59:03.80829Z","shell.execute_reply":"2021-05-23T06:59:03.817913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission.to_csv(\"submission_p2model.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.820813Z","iopub.execute_input":"2021-05-23T06:59:03.82114Z","iopub.status.idle":"2021-05-23T06:59:03.836475Z","shell.execute_reply.started":"2021-05-23T06:59:03.821095Z","shell.execute_reply":"2021-05-23T06:59:03.835094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nBasic lgb\npublic 1.09213\n\nTuned lgbp2>lgbp134\npublic 1.08981\n\nlgbp2 = LGBMClassifier(objective='binary', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.066, \n                       n_estimators=250,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \n                   \n lgbp134 =   LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.1, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.4,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)                \n\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.83793Z","iopub.execute_input":"2021-05-23T06:59:03.838309Z","iopub.status.idle":"2021-05-23T06:59:03.850629Z","shell.execute_reply.started":"2021-05-23T06:59:03.838272Z","shell.execute_reply":"2021-05-23T06:59:03.84945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional2: Summation of features","metadata":{}},{"cell_type":"markdown","source":"Combinatorial summation of the 50 features as new features. The features dimension is exploding with high computation cost. Result is not satisfactory, and not good enough to justify the higher complexity.","metadata":{}},{"cell_type":"code","source":"'''\ntrainsum_features = train_features\ntestsum_features = test_features\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.852434Z","iopub.execute_input":"2021-05-23T06:59:03.852984Z","iopub.status.idle":"2021-05-23T06:59:03.863325Z","shell.execute_reply.started":"2021-05-23T06:59:03.852945Z","shell.execute_reply":"2021-05-23T06:59:03.86226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n%%time\nfor i in range(0,49):\n    a='feature_'+str(i)\n    \n    for j in range(i+1,50): \n        b='feature_'+str(j)\n        trainsum_features[str(i)+\"_\"+str(j)+\"_sum\"] = train_features[[a,b]].sum(axis=1)\n        '''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.865231Z","iopub.execute_input":"2021-05-23T06:59:03.865935Z","iopub.status.idle":"2021-05-23T06:59:03.877378Z","shell.execute_reply.started":"2021-05-23T06:59:03.865885Z","shell.execute_reply":"2021-05-23T06:59:03.8763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n%%time\nfor i in range(0,49):\n    a='feature_'+str(i)\n    \n    for j in range(i+1,50): \n        b='feature_'+str(j)\n        testsum_features[str(i)+\"_\"+str(j)+\"_sum\"] = test_features[[a,b]].sum(axis=1)\n        '''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.879084Z","iopub.execute_input":"2021-05-23T06:59:03.879574Z","iopub.status.idle":"2021-05-23T06:59:03.893573Z","shell.execute_reply.started":"2021-05-23T06:59:03.879518Z","shell.execute_reply":"2021-05-23T06:59:03.892611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n%%time\n# Light Gradient Boosting Regressor\nlgb =  LGBMClassifier(objective='multiclass',                     \n                            verbose=-1,\n                       random_state=17,\n                      importance_type='gain',\n                   n_jobs=-1)\n\nlgb_model = lgb.fit(trainsum_features, train_labels)\nlgb_pred = lgb_model.predict_proba(testsum_features)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.894697Z","iopub.execute_input":"2021-05-23T06:59:03.895022Z","iopub.status.idle":"2021-05-23T06:59:03.909029Z","shell.execute_reply.started":"2021-05-23T06:59:03.894988Z","shell.execute_reply":"2021-05-23T06:59:03.907975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\narr = np.stack((lgb.feature_name_, lgb.feature_importances_) ,axis=1)\nm = [row[0] for row in arr if row[1] != \"0.0\"]\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.91161Z","iopub.execute_input":"2021-05-23T06:59:03.911978Z","iopub.status.idle":"2021-05-23T06:59:03.92337Z","shell.execute_reply.started":"2021-05-23T06:59:03.911937Z","shell.execute_reply":"2021-05-23T06:59:03.922367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntrainsum_features = trainsum_features[m]\ntestsum_features = testsum_features[m]\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.927132Z","iopub.execute_input":"2021-05-23T06:59:03.927492Z","iopub.status.idle":"2021-05-23T06:59:03.935459Z","shell.execute_reply.started":"2021-05-23T06:59:03.927457Z","shell.execute_reply":"2021-05-23T06:59:03.934227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Read in the dataset as a dataframe\ntrainsum_features = pd.read_csv(\"../input/tps-may-2021/trainsum_features.csv\")\ntestsum_features = pd.read_csv(\"../input/tps-may-2021/testsum_features.csv\")\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:03.937571Z","iopub.execute_input":"2021-05-23T06:59:03.937903Z","iopub.status.idle":"2021-05-23T06:59:28.068126Z","shell.execute_reply.started":"2021-05-23T06:59:03.93787Z","shell.execute_reply":"2021-05-23T06:59:28.066981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntrainsum_features.to_csv(\"trainsum_features.csv\", index=False)\ntestsum_features.to_csv(\"testsum_features.csv\", index=False)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:28.069728Z","iopub.execute_input":"2021-05-23T06:59:28.070225Z","iopub.status.idle":"2021-05-23T07:00:06.703185Z","shell.execute_reply.started":"2021-05-23T06:59:28.070172Z","shell.execute_reply":"2021-05-23T07:00:06.701725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Light Gradient Boosting Regressor\nlgbsum =  LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=1, \n                    max_depth=4,\n                      learning_rate=0.07, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                     feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgbsum_model = lgbsum.fit(trainsum_features, train_labels)\nlgbsum_pred = lgbsum_model.predict_proba(testsum_features)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:06.704905Z","iopub.execute_input":"2021-05-23T07:00:06.70537Z","iopub.status.idle":"2021-05-23T07:00:06.710845Z","shell.execute_reply.started":"2021-05-23T07:00:06.705327Z","shell.execute_reply":"2021-05-23T07:00:06.709646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Setup cross validation folds\nkf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n# Define error metrics\ndef loss(y, y_pred):\n    return np.sqrt(log_loss(y, y_pred))\n\ndef cvsum_loss(model, X = trainsum_features):\n    loss = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_log_loss\", cv=kf, n_jobs=-1))\n    return (loss)\n    '''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:06.712517Z","iopub.execute_input":"2021-05-23T07:00:06.713158Z","iopub.status.idle":"2021-05-23T07:00:06.731654Z","shell.execute_reply.started":"2021-05-23T07:00:06.713085Z","shell.execute_reply":"2021-05-23T07:00:06.729682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nscores = {}\n\nscore = cvsum_loss(lgbsum)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:06.733605Z","iopub.execute_input":"2021-05-23T07:00:06.733982Z","iopub.status.idle":"2021-05-23T07:00:06.749659Z","shell.execute_reply.started":"2021-05-23T07:00:06.733946Z","shell.execute_reply":"2021-05-23T07:00:06.748036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n%%time\nnum_est = [0.05,0.07,0.1]\n\nvc_model = LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=1, \n                    max_depth=4,\n                      learning_rate=0.07, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                     feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 3-fold cross validation\ntrain_score, test_score = validation_curve( vc_model,\n                                X = trainsum_features, y = train_labels, \n                                param_name = 'learning_rate', \n                                param_range = num_est, cv = 2, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:30:28.21687Z","iopub.execute_input":"2021-05-23T07:30:28.217323Z","iopub.status.idle":"2021-05-23T07:35:04.926337Z","shell.execute_reply.started":"2021-05-23T07:30:28.21728Z","shell.execute_reply":"2021-05-23T07:35:04.925226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean_test_score","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:35:04.927898Z","iopub.execute_input":"2021-05-23T07:35:04.928221Z","iopub.status.idle":"2021-05-23T07:35:04.934774Z","shell.execute_reply.started":"2021-05-23T07:35:04.928187Z","shell.execute_reply":"2021-05-23T07:35:04.933586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Read in sample_submission dataframe\nsubmission[['Class_1', 'Class_2', 'Class_3', 'Class_4']] = lgbsum_pred\nsubmission.head()\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission.to_csv(\"submission_sumlgb.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nBasic lgb\nCV 1.0478 (0.0006)\npublic 1.09072\n\nTuned lgbsum\nCV 1.0462 (0.0007)\npublic 1.09009\n\nlgbsum = LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=1, \n                    max_depth=4,\n                      learning_rate=0.07, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                     feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n'''","metadata":{},"execution_count":null,"outputs":[]}]}