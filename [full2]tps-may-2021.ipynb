{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesClassifier\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import RidgeClassifier, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import Pool, CatBoostClassifier\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, validation_curve\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Read data","metadata":{}},{"cell_type":"code","source":"# Read in the dataset as a dataframe\ntrain = pd.read_csv(\"../input/tabular-playground-series-may-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-may-2021/test.csv\")\nsubmission = pd.read_csv(\"../input/tabular-playground-series-may-2021/sample_submission.csv\")\n\n#train.info()\n#test.info()\n#submission.info()","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"markdown","source":"## Target distribution","metadata":{}},{"cell_type":"code","source":"'''\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.histplot(train['target'].sort_values(), color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Target\")\nax.set(title=\"Target distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n'''","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'\\nsns.set_style(\"white\")\\nsns.set_color_codes(palette=\\'deep\\')\\nf, ax = plt.subplots(figsize=(8, 7))\\n#Check the new distribution \\nsns.histplot(train[\\'target\\'].sort_values(), color=\"b\");\\nax.xaxis.grid(False)\\nax.set(ylabel=\"Frequency\")\\nax.set(xlabel=\"Target\")\\nax.set(title=\"Target distribution\")\\nsns.despine(trim=True, left=True)\\nplt.show()\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\n# Skew and kurt\nprint(\"Skewness: %f\" % train['Target'].skew())\nprint(\"Kurtosis: %f\" % train['Target'].kurt())\n'''","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'\\n# Skew and kurt\\nprint(\"Skewness: %f\" % train[\\'Target\\'].skew())\\nprint(\"Kurtosis: %f\" % train[\\'Target\\'].kurt())\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Features EDA","metadata":{}},{"cell_type":"code","source":"'''\n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train_features), 1):\n    plt.subplot(len(list(train_features)), 3, i)\n    sns.boxplot(x=feature, y=train_labels, hue=train_labels, palette='Blues', data=train_features)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('Target', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()\n'''","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\n# visualising some more outliers in the data values\\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 120))\\nplt.subplots_adjust(right=2)\\nplt.subplots_adjust(top=2)\\nsns.color_palette(\"husl\", 8)\\nfor i, feature in enumerate(list(train_features), 1):\\n    plt.subplot(len(list(train_features)), 3, i)\\n    sns.boxplot(x=feature, y=train_labels, hue=train_labels, palette=\\'Blues\\', data=train_features)\\n        \\n    plt.xlabel(\\'{}\\'.format(feature), size=15,labelpad=12.5)\\n    plt.ylabel(\\'Target\\', size=15, labelpad=12.5)\\n    \\n    for j in range(2):\\n        plt.tick_params(axis=\\'x\\', labelsize=12)\\n        plt.tick_params(axis=\\'y\\', labelsize=12)\\n    \\n    plt.legend(loc=\\'best\\', prop={\\'size\\': 10})\\n        \\nplt.show()\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Correlation","metadata":{}},{"cell_type":"markdown","source":"Filter by RF feature importance first when the number of features is too large.","metadata":{}},{"cell_type":"code","source":"'''\n# Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\nrf_model = rf.fit(train_features, train_labels)\n#rf_pred = rf_model.predict_proba(test_features)\n\nforest_importances = pd.Series(rf.feature_importances_, index=train_features.columns)\ntop_feat = forest_importances.sort_values(ascending = False).head(20)\ntop_feat\n\ntrain_features[top_feat.index]\n'''","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\n# Random Forest Classifier\\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\\n\\nrf_model = rf.fit(train_features, train_labels)\\n#rf_pred = rf_model.predict_proba(test_features)\\n\\nforest_importances = pd.Series(rf.feature_importances_, index=train_features.columns)\\ntop_feat = forest_importances.sort_values(ascending = False).head(20)\\ntop_feat\\n\\ntrain_features[top_feat.index]\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\n#corr = train_features[top_feat.index].corr()\n#corr\ncorr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)\n'''","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'\\n#corr = train_features[top_feat.index].corr()\\n#corr\\ncorr = train.corr()\\nplt.subplots(figsize=(15,12))\\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Further exploration for high correlation to target","metadata":{}},{"cell_type":"code","source":"'''\ndata = pd.concat([train['feature_38'], train['target']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['feature_38'], y=\"target\", data=data)\n#fig.axis(ymin=0, ymax=800000);\n'''","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\ndata = pd.concat([train[\\'feature_38\\'], train[\\'target\\']], axis=1)\\nf, ax = plt.subplots(figsize=(8, 6))\\nfig = sns.boxplot(x=train[\\'feature_38\\'], y=\"target\", data=data)\\n#fig.axis(ymin=0, ymax=800000);\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\ndata = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0,800000));\n'''","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"\\ndata = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0,800000));\\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# log target if skewed\n# log(1+x) transform\n# train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Remove outliers","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Split datasets","metadata":{}},{"cell_type":"code","source":"# Split features and labels\ntrain_labels = train['target'].reset_index(drop=True)\ntrain_features = train.drop(['id','target'], axis=1)\ntest_features = test.drop(['id'], axis=1)\ntrain_labels.head()","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0    Class_2\n1    Class_1\n2    Class_1\n3    Class_4\n4    Class_2\nName: target, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"'''\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape\n'''","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'\\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\\nall_features.shape\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Missing values","metadata":{}},{"cell_type":"markdown","source":"## Skewed Features","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"'''\n# feature of zero or nonzero values\n\ndef zeroornot(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l] == 0).astype(int)) \n        res.columns.values[m] = l + '_zero'\n        m += 1\n    return res\n\ntrain_features = zeroornot(train_features, train_features.columns.tolist())\ntest_features = zeroornot(test_features, test_features.columns.tolist())\n'''","metadata":{"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"\"\\n# feature of zero or nonzero values\\n\\ndef zeroornot(res, ls):\\n    m = res.shape[1]\\n    for l in ls:\\n        res = res.assign(newcol=pd.Series(res[l] == 0).astype(int)) \\n        res.columns.values[m] = l + '_zero'\\n        m += 1\\n    return res\\n\\ntrain_features = zeroornot(train_features, train_features.columns.tolist())\\ntest_features = zeroornot(test_features, test_features.columns.tolist())\\n\""},"metadata":{}}]},{"cell_type":"code","source":"#train_features = train_features.drop(train_features.iloc[:,0:50], axis=1)\n#test_features = test_features.drop(test_features.iloc[:,0:50], axis=1)","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Encode categorical features","metadata":{}},{"cell_type":"markdown","source":"## Recreate training and test sets","metadata":{}},{"cell_type":"code","source":"'''\nX = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\nX.shape, train_labels.shape, X_test.shape\n'''","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\nX = all_features.iloc[:len(train_labels), :]\\nX_test = all_features.iloc[len(train_labels):, :]\\nX.shape, train_labels.shape, X_test.shape\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"# Model Validation and Selection","metadata":{}},{"cell_type":"code","source":"# Setup cross validation folds\nkf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n# Define error metrics\ndef loss(y, y_pred):\n    return np.sqrt(log_loss(y, y_pred))\n\ndef cv_loss(model, X = train_features):\n    loss = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_log_loss\", cv=kf, n_jobs=-1))\n    return (loss)","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"'''\n# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n                                \n'''","metadata":{"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"\\n# Light Gradient Boosting Regressor\\nlightgbm = LGBMRegressor(objective='regression', \\n                       num_leaves=6,\\n                       learning_rate=0.01, \\n                       n_estimators=7000,\\n                       max_bin=200, \\n                       bagging_fraction=0.8,\\n                       bagging_freq=4, \\n                       bagging_seed=8,\\n                       feature_fraction=0.2,\\n                       feature_fraction_seed=8,\\n                       min_sum_hessian_in_leaf = 11,\\n                       verbose=-1,\\n                       random_state=42)\\n\\n# XGBoost Regressor\\nxgboost = XGBRegressor(learning_rate=0.01,\\n                       n_estimators=6000,\\n                       max_depth=4,\\n                       min_child_weight=0,\\n                       gamma=0.6,\\n                       subsample=0.7,\\n                       colsample_bytree=0.7,\\n                       objective='reg:linear',\\n                       nthread=-1,\\n                       scale_pos_weight=1,\\n                       seed=27,\\n                       reg_alpha=0.00006,\\n                       random_state=42)\\n\\n# Ridge Regressor\\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\\n\\n# Support Vector Regressor\\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\\n\\n# Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(n_estimators=6000,\\n                                learning_rate=0.01,\\n                                max_depth=4,\\n                                max_features='sqrt',\\n                                min_samples_leaf=15,\\n                                min_samples_split=10,\\n                                loss='huber',\\n                                random_state=42)  \\n\\n# Random Forest Regressor\\nrf = RandomForestRegressor(n_estimators=1200,\\n                          max_depth=15,\\n                          min_samples_split=5,\\n                          min_samples_leaf=5,\\n                          max_features=None,\\n                          oob_score=True,\\n                          random_state=42)\\n\\n# Stack up all the models above, optimized using xgboost\\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\\n                                meta_regressor=xgboost,\\n                                use_features_in_secondary=True)\\n                                \\n\""},"metadata":{}}]},{"cell_type":"code","source":"\n# XGBoost Classifier\nxgb = XGBClassifier(learning_rate = 0.1,\n                        colsample_bytree = 0.5,\n                        max_depth = 10,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.9,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n#xgb_model = xgb.fit(train_features, train_labels)\n#xgb_pred = xgb_model.predict_proba(test_features)\n","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\n# XGBoost Classifier2\nxgb2 = XGBClassifier(n_estimators=110,\n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                       max_depth = 2,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n#xgb2_model = xgb2.fit(train_features, train_labels)\n#xgb2_pred = xgb2_model.predict_proba(test_features)","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# XGBoost Classifier final for stacking\n\nxgbf = XGBClassifier(n_estimators=180, \n                        learning_rate = 0.6, \n                        colsample_bytree = 0.7, \n                       max_depth = 1,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                   ","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\n# Random Forest Classifier\nrf = RandomForestClassifier(min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = None,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            criterion = \"entropy\",\n                            n_estimators=500,\n                            max_features = 12,\n                            random_state = 42)\n'''\nrf_model = rf.fit(train_features, train_labels)\nrf_pred = rf_model.predict_proba(test_features)\n'''","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'\\nrf_model = rf.fit(train_features, train_labels)\\nrf_pred = rf_model.predict_proba(test_features)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"\n# Light Gradient Boosting Regressor\nlgb =  LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=220,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb_model = lgb.fit(train_features, train_labels)\n#lgb_pred = lgb_model.predict_proba(test_features)\n","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\n# Extra Trees Classifier\next =ExtraTreesClassifier(  min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = 15,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            n_estimators=10,\n                            max_features = 20,\n                            random_state = 42,\n                            criterion = 'entropy')\n'''\next_model = ext.fit(train_features, train_labels)\next_pred = ext_model.predict_proba(test_features)\n'''","metadata":{"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'\\next_model = ext.fit(train_features, train_labels)\\next_pred = ext_model.predict_proba(test_features)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"\n#CatBoost\ncat_features = train_features.columns.values.tolist()\n\ntrain_dataset = Pool(data=train_features,\n                     label=train_labels,\n                     cat_features=cat_features)\n\neval_dataset = Pool(data=test_features,\n                    cat_features=cat_features)\n\n# Initialize CatBoostClassifier\ncat = CatBoostClassifier(n_estimators=500,\n                           learning_rate=0.3,\n                           max_depth=2,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5)\n# Fit model\n#cat.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat_pred = cat.predict_proba(eval_dataset)\n","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"\n#CatBoost2\ncat_features = train_features.columns.values.tolist()\n\ntrain_dataset = Pool(data=train_features,\n                     label=train_labels,\n                     cat_features=cat_features)\n\neval_dataset = Pool(data=test_features,\n                    cat_features=cat_features)\n\n# Initialize CatBoostClassifier\ncat2 = CatBoostClassifier(n_estimators=550,\n                           learning_rate=0.5,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5)\n# Fit model\n#cat2.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat2_pred = cat2.predict_proba(eval_dataset)","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"%%time\n# Stack up all the models above, optimized using lgb\nstack_gen = StackingCVClassifier(classifiers = (rf, lgb, xgb, ext),\n                                meta_classifier = lgb,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17)\n\n#stack_gen_model = stack_gen.fit(np.array(train_features), np.array(train_labels))\n#stack_pred = stack_gen_model.predict_proba(np.array(test_features))\n","metadata":{"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"CPU times: user 22 µs, sys: 0 ns, total: 22 µs\nWall time: 26.5 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\n%%time\n# Stack2\n# Stack up all the models above, optimized using cat\nstack_gen2 = StackingCVClassifier(classifiers = (lgb, xgb2, cat),\n                                meta_classifier = cat,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17)\n\nstack_gen2_model = stack_gen2.fit(np.array(train_features), np.array(train_labels))\n#stack2_pred = stack_gen2_model.predict_proba(np.array(test_features))\n'''","metadata":{"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'\\n%%time\\n# Stack2\\n# Stack up all the models above, optimized using cat\\nstack_gen2 = StackingCVClassifier(classifiers = (lgb, xgb2, cat),\\n                                meta_classifier = cat,\\n                                 use_probas= True,\\n                                use_features_in_secondary=True, \\n                                 verbose=2,\\n                                 n_jobs=-1,\\n                                random_state=17)\\n\\nstack_gen2_model = stack_gen2.fit(np.array(train_features), np.array(train_labels))\\n#stack2_pred = stack_gen2_model.predict_proba(np.array(test_features))\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\ncat2_pred = cat2.predict_proba(eval_dataset)\nstack_pred = stack_gen_model.predict_proba(np.array(test_features))\nstack2_pred = stack_gen2_model.predict_proba(np.array(test_features))\n'''","metadata":{"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'\\ncat2_pred = cat2.predict_proba(eval_dataset)\\nstack_pred = stack_gen_model.predict_proba(np.array(test_features))\\nstack2_pred = stack_gen2_model.predict_proba(np.array(test_features))\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\ntestf_features = np.append(stack_pred,stack2_pred,axis=1)\ntestf_features = np.append(testf_features,cat2_pred,axis=1)\n'''","metadata":{"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'\\ntestf_features = np.append(stack_pred,stack2_pred,axis=1)\\ntestf_features = np.append(testf_features,cat2_pred,axis=1)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\ncat2_predtr = cat2.predict_proba(train_dataset)\nstack_predtr = stack_gen_model.predict_proba(np.array(train_features))\nstack2_predtr = stack_gen2_model.predict_proba(np.array(train_features))\n'''","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'\\ncat2_predtr = cat2.predict_proba(train_dataset)\\nstack_predtr = stack_gen_model.predict_proba(np.array(train_features))\\nstack2_predtr = stack_gen2_model.predict_proba(np.array(train_features))\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\ntrainf_features = np.append(stack_predtr,stack2_predtr,axis=1)\ntrainf_features = np.append(trainf_features,cat2_predtr,axis=1)\n'''","metadata":{"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'\\ntrainf_features = np.append(stack_predtr,stack2_predtr,axis=1)\\ntrainf_features = np.append(trainf_features,cat2_predtr,axis=1)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\n%%time\n# Stackfinal\n\nxgbf_model = xgbf.fit(trainf_features, train_labels)\nstackf_pred = xgbf_model.predict_proba(testf_features)\n'''","metadata":{"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'\\n%%time\\n# Stackfinal\\n\\nxgbf_model = xgbf.fit(trainf_features, train_labels)\\nstackf_pred = xgbf_model.predict_proba(testf_features)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"\nscores = {}\n\nscore = cv_loss(stack_gen)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())\n","metadata":{"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"rf: 1.0455 (0.0007)\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\ndef cvsf_loss(model, X = trainf_features):\n    loss = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_log_loss\", cv=kf, n_jobs=-1))\n    return (loss)\n\nscores = {}\n\nscore = cvsf_loss(xgbf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.2 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) + \\\n            (0.35 * stack_gen_model.predict(np.array(X))))\n\n# Get final precitions from the blended model\nblended_score = rmsle(train_labels, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)\n\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"#stack_pred2 = stack_gen_model.predict(np.array(train_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#unique, counts = np.unique(stack_pred2, return_counts=True)\n#np.asarray((unique, counts)).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.around(confusion_matrix(train_labels,stack_pred2, normalize = 'pred'),3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"## Validation Curves","metadata":{}},{"cell_type":"code","source":"'''\nnum_est = [180,220,260]\nvc_model = XGBClassifier(n_estimators=180, \n                        learning_rate = 0.6, \n                        colsample_bytree = 0.7, \n                       max_depth = 1,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 3-fold cross validation\ntrain_score, test_score = validation_curve( vc_model,\n                                X = trainf_features, y = train_labels, \n                                param_name = 'n_estimators', \n                                param_range = num_est, cv = 2, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean_test_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Hyperparameter Grid","metadata":{}},{"cell_type":"code","source":"'''\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 10)]\n# Number of features to consider at every split\nmax_features = [int(x) for x in np.linspace(5, 15, num = 10)]\nmax_features.append(\"auto\")\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 10, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = 5\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = 5\n# Method of selecting samples for training each tree\nbootstrap = True\n\n#XGB\nlearning_rate = [0.01,0.1,0.3,0.5, 0.7,1]\ncolsample_bytree = [0.05,0.1, 0.3, 0.5,0.7,1]\n\n#LGB\nfeature_fraction = [0.1, 0.3, 0.5, 0.7, 0.9, 1]\n\n#CAT\ncolsample_bylevel =[0.1, 0.3, 0.5, 0.7, 0.9, 1]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               #'max_features': max_features,\n               'max_depth': max_depth,\n               #'min_samples_split': min_samples_split,\n               #'min_samples_leaf': min_samples_leaf,\n               #'bootstrap': bootstrap,\n               'learning_rate': learning_rate,\n               'colsample_bytree': colsample_bytree\n               #'feature_fraction':feature_fraction\n               #'colsample_bylevel':colsample_bylevel\n              }\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = XGBClassifier(#n_estimators=110,\n                        #learning_rate = 0.5,\n                        #colsample_bytree = 0.13,\n                       #max_depth = 2,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n# Random search of parameters, using 2 fold cross validation, \n# search across 30 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(trainf_features, train_labels)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grid Search","metadata":{}},{"cell_type":"code","source":"'''\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [1,2,3],\n    'n_estimators': [450,500,550],\n    'learning_rate':[0.1,0.3,0.5],\n    'colsample_bylevel':[0.3,0.5,0.7]\n}\n# Create a based model\nrf = CatBoostClassifier(#n_estimators=500,\n                           #learning_rate=0.3,\n                           #max_depth=2,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            #colsample_bylevel=0.5,\n                            min_data_in_leaf=5)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 2, n_jobs = -1, verbose = 2)\n  '''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Fit the grid search to the data\ngrid_search.fit(train_features, train_labels)\n#best_grid = \ngrid_search.best_estimator_\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Read in sample_submission dataframe\nsubmission[['Class_1', 'Class_2', 'Class_3', 'Class_4']] = stack_pred\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission_stackf2.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nPerformance\n\nBasic rf (n=100)\nCV 1.0633 (0.0022)\npublic 1.12521\n\nFeatured rf (n=100)\nCV 1.0643 (0.0024)\npublic 1.12140\n\nTuned rf\nCV 1.0506 (0.0005)\npublic 1.09957\n\nTuned rf with criterion = \"entropy\"\nCV 1.0501 (0.0007)\npublic 1.09912\n\nBasic xgb\npublic 1.09495\n\nTuned xgb\nCV 1.0473 (0.0007)\npublic 1.08944\n\nTuned2 xgb\nCV 1.0453 (0.0006)\npublic 1.08896\n\nBasic lgb\nCV 1.0470 (0.0006)\npublic 1.09036\n\nTuned lgb\nCV 1.0454 (0.0007)\npublic 1.08871\n\nBasic ext\nCV 1.0643 (0.0011)\npublic 1.12962\n\nTuned ext\nCV 1.0550 (0.0004)\npublic 1.10729\n\nTuned2 ext\nCV 1.0536 (0.0005)\npublic 1.10495\n\nStacking (rf, xgb, lgb, ext)>lgb\nCV 1.0455 (0.0007)\npublic 1.08811\n\nBasic cat\nCV 1.0516 (0.0009)\npublic 1.10367\n\nTuned cat\nCV 1.0450 (0.0008)\npublic 1.09039\n\nTuned2 cat\nCV 1.0449 (0.0007)\npublic 1.09092\n\nStacking1 (xgb, lgb, cat)>lgb\nCV 1.0450 (0.0008)\npublic 1.08605\n\nStacking2 (xgb2, lgb, cat)>cat\nCV 1.0448 (0.0008)\npublic 1.08766\n\nStacking3 (xgb2, lgb, cat)>xgb2\nCV 1.0458 (0.0008)\npublic 1.08729\n\nStackingf (Stacking1, Stacking2, cat2)>xgb2\nCV 1.0272 (0.0009)\npublic 1.09985\n\nStackingf2 (Stacking1, Stacking2, cat2)>xgbf\nCV 1.0216 (0.0008)\npublic 1.11130\n\n\nrf = RandomForestClassifier(min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = None,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            criterion = \"entropy\",\n                            n_estimators=500,\n                            max_features = 12,\n                            random_state = 42)\n\nxgb1 = XGBClassifier(learning_rate = 0.1,\n                        colsample_bytree = 0.5,\n                        max_depth = 10,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.9,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                       \nxgb2 = XGBClassifier(n_estimators=110,\n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                       max_depth = 2,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                       \n                       \nlgb = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=220,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \next = ExtraTreesClassifier(  min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = 15,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            n_estimators=10,\n                            max_features = 20,\n                            random_state = 42,\n                            criterion = 'entropy')\n                            \n\ncat = CatBoostClassifier(n_estimators=500,\n                           learning_rate=0.3,\n                           max_depth=2,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5)\n                            \n                            \ncat2 = CatBoostClassifier(n_estimators=550,\n                           learning_rate=0.5,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5)\n                            \nxgbf = XGBClassifier(n_estimators=180, \n                        learning_rate = 0.6, \n                        colsample_bytree = 0.7, \n                       max_depth = 1,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                   \n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}